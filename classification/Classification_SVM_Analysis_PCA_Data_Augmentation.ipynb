{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:48.175489Z","iopub.status.busy":"2022-04-06T09:58:48.174964Z","iopub.status.idle":"2022-04-06T09:58:49.202291Z","shell.execute_reply":"2022-04-06T09:58:49.201673Z","shell.execute_reply.started":"2022-04-06T09:58:48.175384Z"},"trusted":true},"outputs":[],"source":["# importing necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn.metrics import plot_confusion_matrix\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import MinMaxScaler \n","from keras.layers import Input, Dense\n","from keras.models import Model, Sequential\n","from keras import regularizers"]},{"cell_type":"markdown","metadata":{},"source":["# Load the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:50.047165Z","iopub.status.busy":"2022-04-06T09:58:50.046806Z","iopub.status.idle":"2022-04-06T09:58:50.103494Z","shell.execute_reply":"2022-04-06T09:58:50.10259Z","shell.execute_reply.started":"2022-04-06T09:58:50.047137Z"},"trusted":true},"outputs":[],"source":["# reading tweets data\n","df=pd.read_csv('../input/bitcoin-tweets/train_data_inference.csv')\n","df_test=pd.read_csv('../input/bitcoin-tweets/test_data_inference.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df=df.drop(['Unnamed: 0','username', 'date', 'country', 'replyCount', 'retweetCount', 'likeCount', 'url', 'textblob_class', 'vader_class'],axis=1)\n","df_new = df\n","df= df.drop(['content', 'inference'], axis =1)\n","df_test=df_test.drop(['Unnamed: 0','username', 'date', 'country', 'inference', 'content','replyCount', 'retweetCount', 'likeCount', 'url', 'textblob_class', 'vader_class'],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:50.135149Z","iopub.status.busy":"2022-04-06T09:58:50.134575Z","iopub.status.idle":"2022-04-06T09:58:50.145476Z","shell.execute_reply":"2022-04-06T09:58:50.144847Z","shell.execute_reply.started":"2022-04-06T09:58:50.135112Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:50.146843Z","iopub.status.busy":"2022-04-06T09:58:50.146576Z","iopub.status.idle":"2022-04-06T09:58:50.157829Z","shell.execute_reply":"2022-04-06T09:58:50.156792Z","shell.execute_reply.started":"2022-04-06T09:58:50.146813Z"},"trusted":true},"outputs":[],"source":["df_new.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:50.909631Z","iopub.status.busy":"2022-04-06T09:58:50.90934Z","iopub.status.idle":"2022-04-06T09:58:50.942041Z","shell.execute_reply":"2022-04-06T09:58:50.941489Z","shell.execute_reply.started":"2022-04-06T09:58:50.909593Z"},"trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T09:58:51.154532Z","iopub.status.busy":"2022-04-06T09:58:51.153815Z","iopub.status.idle":"2022-04-06T09:58:51.167051Z","shell.execute_reply":"2022-04-06T09:58:51.166137Z","shell.execute_reply.started":"2022-04-06T09:58:51.154483Z"},"trusted":true},"outputs":[],"source":["# getting value counts for each sentiment in train df\n","print(\"Label   Count\")\n","df['sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["# SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:38:13.037927Z","iopub.status.busy":"2022-04-06T08:38:13.037352Z","iopub.status.idle":"2022-04-06T08:38:13.162936Z","shell.execute_reply":"2022-04-06T08:38:13.161958Z","shell.execute_reply.started":"2022-04-06T08:38:13.03789Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","# create feature vectors using tfidf\n","vectorizer = TfidfVectorizer(min_df = 5,\n","                             max_df = 0.8,\n","                             sublinear_tf = True,\n","                             use_idf = True)\n","train_vectors = vectorizer.fit_transform(df['processed_content'])\n","test_vectors = vectorizer.transform(df_test['processed_content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:38:17.413278Z","iopub.status.busy":"2022-04-06T08:38:17.412972Z","iopub.status.idle":"2022-04-06T08:38:17.418397Z","shell.execute_reply":"2022-04-06T08:38:17.417755Z","shell.execute_reply.started":"2022-04-06T08:38:17.413239Z"},"trusted":true},"outputs":[],"source":["# fit the svm models\n","linear = svm.SVC(kernel='linear').fit(train_vectors, df['sentiment'])\n","rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(train_vectors, df['sentiment'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:38:25.521953Z","iopub.status.busy":"2022-04-06T08:38:25.521683Z","iopub.status.idle":"2022-04-06T08:38:27.365054Z","shell.execute_reply":"2022-04-06T08:38:27.364168Z","shell.execute_reply.started":"2022-04-06T08:38:25.521924Z"},"trusted":true},"outputs":[],"source":["# make predictions\n","linear_pred = linear.predict(test_vectors)\n","poly_pred = poly.predict(test_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:42:33.693186Z","iopub.status.busy":"2022-04-06T08:42:33.692029Z","iopub.status.idle":"2022-04-06T08:42:33.711007Z","shell.execute_reply":"2022-04-06T08:42:33.709742Z","shell.execute_reply.started":"2022-04-06T08:42:33.693105Z"},"trusted":true},"outputs":[],"source":["# retrieve the accuracy and print it for all 4 kernel functions\n","accuracy_score=round(accuracy_score(linear_pred,df_test['sentiment']), 3)\n","precision = round(precision_score(df_test['sentiment'],linear_pred, average=\"weighted\"), 3)\n","recall = round(recall_score(df_test['sentiment'],linear_pred, average=\"weighted\"), 3)\n","f1_score = round(f1_score(df_test['sentiment'], linear_pred, average=\"weighted\"), 3)\n","\n","print('Linear Kernel:')\n","print('Accuracy:   ', accuracy_score)\n","print('Precision:  ', precision)\n","print('Recall:     ', recall)\n","print('F1 Score:   ', f1_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:44:59.25065Z","iopub.status.busy":"2022-04-06T08:44:59.249581Z","iopub.status.idle":"2022-04-06T08:44:59.267112Z","shell.execute_reply":"2022-04-06T08:44:59.266192Z","shell.execute_reply.started":"2022-04-06T08:44:59.250594Z"},"trusted":true},"outputs":[],"source":["# retrieve the accuracy and print it for all 4 kernel functions\n","accuracy_score=round(accuracy_score(rbf_pred,df_test['sentiment']), 3)\n","precision = round(precision_score(df_test['sentiment'],rbf_pred, average=\"weighted\"), 3)\n","recall = round(recall_score(df_test['sentiment'],rbf_pred, average=\"weighted\"), 3)\n","f1_score = round(f1_score(df_test['sentiment'], rbf_pred, average=\"weighted\"), 3)\n","\n","print('RBF Kernel:')\n","print('Accuracy:   ', accuracy_score)\n","print('Precision:  ', precision)\n","print('Recall:     ', recall)\n","print('F1 Score:   ', f1_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:55:14.386495Z","iopub.status.busy":"2022-04-06T08:55:14.386196Z","iopub.status.idle":"2022-04-06T08:55:15.115522Z","shell.execute_reply":"2022-04-06T08:55:15.114973Z","shell.execute_reply.started":"2022-04-06T08:55:14.386466Z"},"trusted":true},"outputs":[],"source":["# plot confusion matrix\n","plot_confusion_matrix(linear, test_vectors, df_test['sentiment']) \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T08:55:29.799525Z","iopub.status.busy":"2022-04-06T08:55:29.798633Z","iopub.status.idle":"2022-04-06T08:55:30.50417Z","shell.execute_reply":"2022-04-06T08:55:30.503254Z","shell.execute_reply.started":"2022-04-06T08:55:29.799471Z"},"trusted":true},"outputs":[],"source":["plot_confusion_matrix(rbf, test_vectors, df_test['sentiment']) \n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Enhancement models and analysis"]},{"cell_type":"markdown","metadata":{},"source":["## 1. PCA with basic classifiers\n","##### code reference: https://www.kaggle.com/code/tomras/sentiment-analysis-of-tweets-using-pca-and-ml/notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:02:33.799623Z","iopub.status.busy":"2022-04-06T10:02:33.79931Z","iopub.status.idle":"2022-04-06T10:02:33.887209Z","shell.execute_reply":"2022-04-06T10:02:33.886399Z","shell.execute_reply.started":"2022-04-06T10:02:33.799577Z"},"trusted":true},"outputs":[],"source":["# create feature vectors using tfidf\n","vectorizer = TfidfVectorizer()\n","text_features_train = vectorizer.fit_transform(df['processed_content'])\n","text_features_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:05:23.621115Z","iopub.status.busy":"2022-04-06T10:05:23.620839Z","iopub.status.idle":"2022-04-06T10:05:25.438848Z","shell.execute_reply":"2022-04-06T10:05:25.438132Z","shell.execute_reply.started":"2022-04-06T10:05:23.621087Z"},"trusted":true},"outputs":[],"source":["# declaring PCA with 3 components\n","pca = PCA(n_components=3)\n","features_train = pca.fit_transform(text_features_train.toarray())\n","features_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:05:26.323213Z","iopub.status.busy":"2022-04-06T10:05:26.322348Z","iopub.status.idle":"2022-04-06T10:05:26.348574Z","shell.execute_reply":"2022-04-06T10:05:26.347856Z","shell.execute_reply.started":"2022-04-06T10:05:26.32317Z"},"trusted":true},"outputs":[],"source":["# adding pca components to df\n","df_features_train = pd.DataFrame(features_train)\n","df_features_train = pd.concat([df_features_train, df[['sentiment']]], axis=1, ignore_index=True)\n","df_features_train.columns = ['pca_1', 'pca_2', 'pca_3', 'target'] #'pca_3',\n","df_features_train.describe(include='all')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:24:56.530507Z","iopub.status.busy":"2022-04-06T10:24:56.530044Z","iopub.status.idle":"2022-04-06T10:24:56.785305Z","shell.execute_reply":"2022-04-06T10:24:56.784325Z","shell.execute_reply.started":"2022-04-06T10:24:56.530467Z"},"trusted":true},"outputs":[],"source":["# plotting the scatter plot to see how we separated are the 3 sentiments in the extracted PCA components\n","cmap = {0: 'red', 1: 'blue', -1: 'green'}\n","df_features_train.plot(kind='scatter', x='pca_1', y='pca_2', c=[cmap.get(t, 'black') for t in df_features_train['target']])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:08:09.330224Z","iopub.status.busy":"2022-04-06T10:08:09.329648Z","iopub.status.idle":"2022-04-06T10:08:09.334911Z","shell.execute_reply":"2022-04-06T10:08:09.333999Z","shell.execute_reply.started":"2022-04-06T10:08:09.330188Z"},"trusted":true},"outputs":[],"source":["# for reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:24:09.565297Z","iopub.status.busy":"2022-04-06T10:24:09.565037Z","iopub.status.idle":"2022-04-06T10:24:10.084772Z","shell.execute_reply":"2022-04-06T10:24:10.084195Z","shell.execute_reply.started":"2022-04-06T10:24:09.565269Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"pca_1\", y=\"pca_2\",\n","    hue=\"target\",\n","    data=df_features_train.loc[rndperm,:],\n","    legend=\"full\",\n","    alpha=0.3\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-06T10:10:03.934546Z","iopub.status.busy":"2022-04-06T10:10:03.934261Z","iopub.status.idle":"2022-04-06T10:10:04.374053Z","shell.execute_reply":"2022-04-06T10:10:04.373081Z","shell.execute_reply.started":"2022-04-06T10:10:03.934501Z"},"trusted":true},"outputs":[],"source":["# 3d plot\n","ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n","ax.scatter(\n","    xs=df_features_train.loc[rndperm,:][\"pca_1\"], \n","    ys=df_features_train.loc[rndperm,:][\"pca_2\"], \n","    zs=df_features_train.loc[rndperm,:][\"pca_3\"], \n","    c=df.loc[rndperm,:][\"sentiment\"], \n","    cmap='tab10'\n",")\n","ax.set_xlabel('pca-one')\n","ax.set_ylabel('pca-two')\n","ax.set_zlabel('pca-three')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Analysing test data misclassifications"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count = 0\n","for i, j, index in zip(df_new.sentiment, df_new.inference, df_new.index):\n","    if(int(i) == int(j)):\n","        df_new = df_new.drop(labels=[index], axis=0)\n","        count = count +1\n","print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(df_new)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.processed_content, df_new.index):\n","    if 'bearish' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.content, df_new.index):\n","    if 'bearish' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.processed_content, df_new.index):\n","    if 'fall' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.content, df_new.index):\n","    if 'fall' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.processed_content, df_new.index):\n","    if 'down' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.content, df_new.index):\n","    if 'down' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.processed_content, df_new.index):\n","    if 'volatility' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for content, ind in zip(df_new.content, df_new.index):\n","    if 'volatility' in content:\n","        print(df_new.sentiment[ind], df_new.inference[ind])"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Data augmentation\n","##### code reference: https://github.com/kothiyayogesh/medium-article-code/blob/master/How%20I%20dealt%20with%20Imbalanced%20text%20dataset/data_augmentation_using_word_embedding.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:46:32.6188Z","iopub.status.busy":"2022-04-03T15:46:32.618547Z","iopub.status.idle":"2022-04-03T15:46:32.647915Z","shell.execute_reply":"2022-04-03T15:46:32.647157Z","shell.execute_reply.started":"2022-04-03T15:46:32.618771Z"},"trusted":true},"outputs":[],"source":["tqdm.pandas()\n","np.random.seed(100)\n","# read file\n","file_name = '../input/bitcoin-tweets/train_data_inference.csv'\n","# read file using pandas\n","df = pd.read_csv(file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:28:40.975186Z","iopub.status.busy":"2022-04-03T15:28:40.974657Z","iopub.status.idle":"2022-04-03T15:28:40.99181Z","shell.execute_reply":"2022-04-03T15:28:40.990793Z","shell.execute_reply.started":"2022-04-03T15:28:40.975149Z"},"trusted":true},"outputs":[],"source":["def loadEmbeddingMatrix(typeToLoad, vocab_dict):\n","    import gensim.models.keyedvectors as word2vec\n","    import gc\n","\n","    # load different embedding file from Kaggle depending on which embedding\n","    # matrix we are going to experiment with\n","    if (typeToLoad == \"gloveTwitter50d\"):\n","        EMBEDDING_FILE = 'embeddings\\glove-twitter-27b-50d/glove.twitter.27B.50d.txt'\n","        embed_size = 50\n","    elif (typeToLoad == \"word2vec\"):\n","        word2vecDict = word2vec.KeyedVectors.load_word2vec_format( \"embeddings\\GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)\n","        embed_size = 300\n","    elif (typeToLoad == \"fasttext\"):\n","        EMBEDDING_FILE = 'embeddings\\\\fasttext/wiki.simple.vec'\n","        embed_size = 300\n","    elif (typeToLoad == \"glove840B300D\"):\n","        EMBEDDING_FILE = '../input/embeddings/glove.840B.300d.txt'\n","        embed_size = 300\n","    elif (typeToLoad == \"glove6B300D\"):\n","        EMBEDDING_FILE = 'embeddings\\glove.6B\\glove.6B.300d.txt'\n","        embed_size = 300\n","    elif (typeToLoad == \"paragram\"):\n","        EMBEDDING_FILE = 'embeddings\\paragram_300_sl999\\paragram_300_sl999.txt'\n","        embed_size = 300\n","    elif (typeToLoad == \"wikiNews\"):\n","        EMBEDDING_FILE = \"embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec\"\n","        embed_size = 300\n","\n","    def get_coefs(word, *arr):\n","        return word, np.asarray(arr, dtype='float32')\n","\n","    if (typeToLoad in [\"gloveTwitter50d\", \"fasttext\"]):\n","        embeddings_index = dict()\n","        # Transfer the embedding weights into a dictionary by iterating through every line of the file.\n","        f = open(EMBEDDING_FILE)\n","        for line in f:\n","            # split up line into an indexed array\n","            values = line.rstrip().rsplit(' ')  # line.split()\n","            # first index is word\n","            word = values[0]\n","            # store the rest of the values in the array as a new array\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs  # 50 dimensions\n","        f.close()\n","    elif (typeToLoad in [\"glove840B300D\", \"paragram\", \"glove6B300D\"]):\n","        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='latin'))\n","    elif (typeToLoad in [\"wikiNews\"]):\n","        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o) > 100)\n","    else:\n","        embeddings_index = dict()\n","        for word in word2vecDict.wv.vocab:\n","            embeddings_index[word] = word2vecDict.word_vec(word)\n","    print('Loaded %s word vectors.' % len(embeddings_index))\n","\n","    gc.collect()\n","    # We get the mean and standard deviation of the embedding weights so that we could maintain the\n","    # same statistics for the rest of our own random generated weights.\n","    all_embs = np.stack(list(embeddings_index.values()))\n","    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n","\n","    nb_words = len(vocab_dict)\n","    # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n","    # the size will be Number of Words in Vocab X Embedding Size\n","    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n","    gc.collect()\n","\n","    # With the newly created embedding matrix, we'll fill it up with the words that we have in both\n","    # our own dictionary and loaded pretrained embedding.\n","    embeddedCount = 0\n","    for word, i in vocab_dict.items():\n","        #i -= 1\n","        # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n","        embedding_vector = embeddings_index.get(word)\n","        # and store inside the embedding matrix that we will train later on.\n","        if embedding_vector is not None:\n","            try :\n","                embedding_matrix[i] = embedding_vector\n","                embeddedCount += 1\n","            except IndexError:\n","                pass\n","    print('total embedded:', embeddedCount, 'common words')\n","\n","    del embeddings_index\n","    gc.collect()\n","\n","    # finally return the embedding matrix\n","    return embedding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:28:41.229768Z","iopub.status.busy":"2022-04-03T15:28:41.229279Z","iopub.status.idle":"2022-04-03T15:28:46.032091Z","shell.execute_reply":"2022-04-03T15:28:46.031314Z","shell.execute_reply.started":"2022-04-03T15:28:41.229727Z"},"trusted":true},"outputs":[],"source":["# tokenizing sentence for finding synonym\n","def make_tokenizer(texts):\n","    from keras.preprocessing.text import Tokenizer\n","    t = Tokenizer()\n","    t.fit_on_texts(texts)\n","    return t\n","\n","tokenizer = make_tokenizer(df['content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:28:46.03393Z","iopub.status.busy":"2022-04-03T15:28:46.033668Z","iopub.status.idle":"2022-04-03T15:28:46.041241Z","shell.execute_reply":"2022-04-03T15:28:46.040552Z","shell.execute_reply.started":"2022-04-03T15:28:46.033895Z"},"trusted":true},"outputs":[],"source":["# dictionary of word index\n","index_word = {}\n","for word in tokenizer.word_index.keys():\n","    index_word[tokenizer.word_index[word]] = word\n","\n","vocab_dict = tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:32:17.774038Z","iopub.status.busy":"2022-04-03T15:32:17.773484Z","iopub.status.idle":"2022-04-03T15:35:12.456383Z","shell.execute_reply":"2022-04-03T15:35:12.455514Z","shell.execute_reply.started":"2022-04-03T15:32:17.773999Z"},"trusted":true},"outputs":[],"source":["# loading word embedding\n","embed_mat = loadEmbeddingMatrix(\"glove840B300D\", vocab_dict)\n","print(\"Embedding loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:35:12.458456Z","iopub.status.busy":"2022-04-03T15:35:12.458128Z","iopub.status.idle":"2022-04-03T15:35:16.606795Z","shell.execute_reply":"2022-04-03T15:35:16.606054Z","shell.execute_reply.started":"2022-04-03T15:35:12.458419Z"},"trusted":true},"outputs":[],"source":["from sklearn.neighbors import NearestNeighbors\n","\n","synonyms_number = 5\n","word_number = 20000\n","\n","nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat)\n","\n","neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n","\n","synonyms = {x[0]: x[1:] for x in neighbours_mat}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:35:16.608455Z","iopub.status.busy":"2022-04-03T15:35:16.608114Z","iopub.status.idle":"2022-04-03T15:35:16.614771Z","shell.execute_reply":"2022-04-03T15:35:16.613876Z","shell.execute_reply.started":"2022-04-03T15:35:16.608417Z"},"trusted":true},"outputs":[],"source":["# finding nearby synonym - Basically it's not actually synonym. It's near by words of targetted word. \n","import nltk\n","from nltk.corpus import wordnet\n","\n","synonym = {}\n","for x in range(0,100):\n","    try :\n","        synonym.update({index_word[x] : [index_word[synonyms[x][i]] for i in range(synonyms_number-1)]})\n","    except :\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:35:16.617155Z","iopub.status.busy":"2022-04-03T15:35:16.616843Z","iopub.status.idle":"2022-04-03T15:35:16.625132Z","shell.execute_reply":"2022-04-03T15:35:16.624222Z","shell.execute_reply.started":"2022-04-03T15:35:16.617117Z"},"trusted":true},"outputs":[],"source":["# can only change words for selected part of speech to preserve semantic meaning.\n","\n","import nltk\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","\n","def get_pos_tag (word, tagged) :\n","    res = [(x, y) for x, y in tagged if x == word]\n","    return res[0][1]\n","\n","# Load the pretrained neural net\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:03.179445Z","iopub.status.busy":"2022-04-03T15:52:03.178934Z","iopub.status.idle":"2022-04-03T15:52:06.653511Z","shell.execute_reply":"2022-04-03T15:52:06.652806Z","shell.execute_reply.started":"2022-04-03T15:52:03.179398Z"},"trusted":true},"outputs":[],"source":["dict_syn = {}\n","\n","for message, ind, senti in zip(df[\"content\"], df.index, df['sentiment']):\n","    temp_dict={}\n","    count = 0\n","    if(senti == 1): # choose only pos tweets\n","        # Tokenize the text\n","        tokenized = tokenizer.tokenize(message)\n","\n","        # Get the list of words from the entire text\n","        words = word_tokenize(message)\n","\n","        # Identify the parts of speech\n","        tagged = nltk.pos_tag(words, tagset=\"universal\")\n","\n","        replacements = []\n","\n","        for word in words:\n","            synonym = []\n","            antonyms = []\n","            word_index = vocab_dict.get(word, None)\n","\n","            pos_tag = get_pos_tag(word, tagged)\n","            if (word_index and pos_tag in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] and word not in nltk.corpus.stopwords.words('english')) :\n","                for syn in wordnet.synsets(word, eval(\"wordnet.\" + pos_tag)):\n","                    for l in syn.lemmas() :\n","                        if(l.name() in [index_word[synonyms[word_index][i]] for i in range(synonyms_number-1)]):\n","                            synonym.append(l.name())\n","\n","            if (synonym) :\n","                count = 1\n","                temp_dict.update({word: set(synonym)})\n","\n","        if count == 1:\n","            dict_syn.update( {ind : temp_dict} )\n","\n","              "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:12.448636Z","iopub.status.busy":"2022-04-03T15:52:12.448357Z","iopub.status.idle":"2022-04-03T15:52:12.453978Z","shell.execute_reply":"2022-04-03T15:52:12.453299Z","shell.execute_reply.started":"2022-04-03T15:52:12.448606Z"},"trusted":true},"outputs":[],"source":["tweets_to_make = 1539-760\n","tweets_to_make"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:14.148639Z","iopub.status.busy":"2022-04-03T15:52:14.14811Z","iopub.status.idle":"2022-04-03T15:52:14.156346Z","shell.execute_reply":"2022-04-03T15:52:14.155586Z","shell.execute_reply.started":"2022-04-03T15:52:14.148591Z"},"trusted":true},"outputs":[],"source":["list_new_tweets = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:22.958434Z","iopub.status.busy":"2022-04-03T15:52:22.957658Z","iopub.status.idle":"2022-04-03T15:52:22.982076Z","shell.execute_reply":"2022-04-03T15:52:22.981258Z","shell.execute_reply.started":"2022-04-03T15:52:22.95839Z"},"trusted":true},"outputs":[],"source":["# generating augmented tweets from the synonym dict created earlier\n","flag = 0\n","while(flag==0):\n","    for k in list(dict_syn):\n","        count = 0\n","        if tweets_to_make >0:\n","            for i in dict_syn[k]:\n","                if len(dict_syn[k][i]) >1:\n","                    count = 1\n","\n","            tweet = df['content'][k]\n","            for i in dict_syn[k]:\n","                word_to_replace = i\n","                if count == 1:\n","                    if len(dict_syn[k][i]) >1 :\n","                        tweet = tweet.replace(word_to_replace, dict_syn[k][i].pop())\n","                    else: # set has only one value\n","                        tweet = tweet.replace(word_to_replace, next(iter(dict_syn[k][i])))\n","\n","                if count == 0:\n","                    tweet = tweet.replace(word_to_replace, dict_syn[k][i].pop())\n","\n","            if count == 0:\n","                del dict_syn[k]\n","\n","            tweets_to_make = tweets_to_make -1\n","            list_new_tweets.append(tweet)\n","            if(len(dict_syn)==0):\n","                flag =1\n","                break\n","        else:\n","            flag = 1\n","            break\n","    \n","    print(\"Remaining tweets: \", tweets_to_make)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:37.379017Z","iopub.status.busy":"2022-04-03T15:52:37.378136Z","iopub.status.idle":"2022-04-03T15:52:37.383968Z","shell.execute_reply":"2022-04-03T15:52:37.38307Z","shell.execute_reply.started":"2022-04-03T15:52:37.378958Z"},"trusted":true},"outputs":[],"source":["df_aug_pos = pd.DataFrame(list_new_tweets, columns=['content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:44.829166Z","iopub.status.busy":"2022-04-03T15:52:44.828487Z","iopub.status.idle":"2022-04-03T15:52:44.833871Z","shell.execute_reply":"2022-04-03T15:52:44.833098Z","shell.execute_reply.started":"2022-04-03T15:52:44.829124Z"},"trusted":true},"outputs":[],"source":["df_aug_pos['sentiment'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:52:51.678641Z","iopub.status.busy":"2022-04-03T15:52:51.678364Z","iopub.status.idle":"2022-04-03T15:52:51.686773Z","shell.execute_reply":"2022-04-03T15:52:51.686091Z","shell.execute_reply.started":"2022-04-03T15:52:51.67861Z"},"trusted":true},"outputs":[],"source":["df_aug_pos.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:53:25.901693Z","iopub.status.busy":"2022-04-03T15:53:25.901429Z","iopub.status.idle":"2022-04-03T15:53:28.557268Z","shell.execute_reply":"2022-04-03T15:53:28.556552Z","shell.execute_reply.started":"2022-04-03T15:53:25.901664Z"},"trusted":true},"outputs":[],"source":["dict_syn = {}\n","\n","for message, ind, senti in zip(df[\"content\"], df.index, df['sentiment']):\n","    temp_dict={}\n","    count = 0\n","    if(senti == -1): # choose only pos tweets\n","        # Tokenize the text\n","        tokenized = tokenizer.tokenize(message)\n","\n","        # Get the list of words from the entire text\n","        words = word_tokenize(message)\n","\n","        # Identify the parts of speech\n","        tagged = nltk.pos_tag(words, tagset=\"universal\")\n","\n","        replacements = []\n","\n","        for word in words:\n","            synonym = []\n","            antonyms = []\n","            word_index = vocab_dict.get(word, None)\n","\n","            pos_tag = get_pos_tag(word, tagged)\n","            if (word_index and pos_tag in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] and word not in nltk.corpus.stopwords.words('english')) :\n","                for syn in wordnet.synsets(word, eval(\"wordnet.\" + pos_tag)):\n","                    for l in syn.lemmas() :\n","                        if(l.name() in [index_word[synonyms[word_index][i]] for i in range(synonyms_number-1)]):\n","                            synonym.append(l.name())\n","\n","            if (synonym) :\n","                count = 1\n","                temp_dict.update({word: set(synonym)})\n","\n","        if count == 1:\n","            dict_syn.update( {ind : temp_dict} )\n","              "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:53:40.098858Z","iopub.status.busy":"2022-04-03T15:53:40.098581Z","iopub.status.idle":"2022-04-03T15:53:40.107854Z","shell.execute_reply":"2022-04-03T15:53:40.107109Z","shell.execute_reply.started":"2022-04-03T15:53:40.098807Z"},"trusted":true},"outputs":[],"source":["df['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:54:02.596075Z","iopub.status.busy":"2022-04-03T15:54:02.595516Z","iopub.status.idle":"2022-04-03T15:54:02.601407Z","shell.execute_reply":"2022-04-03T15:54:02.60069Z","shell.execute_reply.started":"2022-04-03T15:54:02.596036Z"},"trusted":true},"outputs":[],"source":["tweets_to_make = 1539-535\n","tweets_to_make"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:54:10.328545Z","iopub.status.busy":"2022-04-03T15:54:10.328279Z","iopub.status.idle":"2022-04-03T15:54:10.332323Z","shell.execute_reply":"2022-04-03T15:54:10.331615Z","shell.execute_reply.started":"2022-04-03T15:54:10.328516Z"},"trusted":true},"outputs":[],"source":["list_new_tweets = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:54:48.371094Z","iopub.status.busy":"2022-04-03T15:54:48.370733Z","iopub.status.idle":"2022-04-03T15:54:48.403828Z","shell.execute_reply":"2022-04-03T15:54:48.402923Z","shell.execute_reply.started":"2022-04-03T15:54:48.371056Z"},"trusted":true},"outputs":[],"source":["flag = 0\n","while(flag==0):\n","    for k in list(dict_syn):\n","        count = 0\n","        if tweets_to_make >0:\n","            for i in dict_syn[k]:\n","                if len(dict_syn[k][i]) >1:\n","                    count = 1\n","\n","            tweet = df['content'][k]\n","            for i in dict_syn[k]:\n","                word_to_replace = i\n","                if count == 1:\n","                    if len(dict_syn[k][i]) >1 :\n","                        tweet = tweet.replace(word_to_replace, dict_syn[k][i].pop())\n","                    else: # set has only one value\n","                        tweet = tweet.replace(word_to_replace, next(iter(dict_syn[k][i])))\n","\n","                if count == 0:\n","                    tweet = tweet.replace(word_to_replace, dict_syn[k][i].pop())\n","\n","            if count == 0:\n","                del dict_syn[k]\n","\n","            tweets_to_make = tweets_to_make -1\n","            list_new_tweets.append(tweet)\n","            if(len(dict_syn)==0):\n","                flag =1\n","                break\n","        else:\n","            flag = 1\n","            break\n","    \n","    print(\"Remaining tweets: \", tweets_to_make)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:55:38.218727Z","iopub.status.busy":"2022-04-03T15:55:38.218439Z","iopub.status.idle":"2022-04-03T15:55:38.223218Z","shell.execute_reply":"2022-04-03T15:55:38.222563Z","shell.execute_reply.started":"2022-04-03T15:55:38.218696Z"},"trusted":true},"outputs":[],"source":["df_aug_neg = pd.DataFrame(list_new_tweets, columns=['content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:55:48.977264Z","iopub.status.busy":"2022-04-03T15:55:48.976627Z","iopub.status.idle":"2022-04-03T15:55:48.98168Z","shell.execute_reply":"2022-04-03T15:55:48.981045Z","shell.execute_reply.started":"2022-04-03T15:55:48.977224Z"},"trusted":true},"outputs":[],"source":["df_aug_neg['sentiment'] = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:55:59.718261Z","iopub.status.busy":"2022-04-03T15:55:59.717931Z","iopub.status.idle":"2022-04-03T15:55:59.726775Z","shell.execute_reply":"2022-04-03T15:55:59.72612Z","shell.execute_reply.started":"2022-04-03T15:55:59.718229Z"},"trusted":true},"outputs":[],"source":["df_aug_neg.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:01:04.447701Z","iopub.status.busy":"2022-04-03T16:01:04.447137Z","iopub.status.idle":"2022-04-03T16:01:04.453172Z","shell.execute_reply":"2022-04-03T16:01:04.452481Z","shell.execute_reply.started":"2022-04-03T16:01:04.447659Z"},"trusted":true},"outputs":[],"source":["# concatenating the positive and negative augmented tweets\n","df_augmented = pd.concat([df_aug_pos, df_aug_neg], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:01:05.286947Z","iopub.status.busy":"2022-04-03T16:01:05.286498Z","iopub.status.idle":"2022-04-03T16:01:05.295384Z","shell.execute_reply":"2022-04-03T16:01:05.294743Z","shell.execute_reply.started":"2022-04-03T16:01:05.286909Z"},"trusted":true},"outputs":[],"source":["df_augmented.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:01:07.774612Z","iopub.status.busy":"2022-04-03T16:01:07.773981Z","iopub.status.idle":"2022-04-03T16:01:07.781684Z","shell.execute_reply":"2022-04-03T16:01:07.780866Z","shell.execute_reply.started":"2022-04-03T16:01:07.774472Z"},"trusted":true},"outputs":[],"source":["df_augmented['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:02:40.521383Z","iopub.status.busy":"2022-04-03T16:02:40.520818Z","iopub.status.idle":"2022-04-03T16:02:40.538701Z","shell.execute_reply":"2022-04-03T16:02:40.538045Z","shell.execute_reply.started":"2022-04-03T16:02:40.521344Z"},"trusted":true},"outputs":[],"source":["df_augmented.to_csv(\"./train_augmented_only.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:06:43.926332Z","iopub.status.busy":"2022-04-03T16:06:43.925532Z","iopub.status.idle":"2022-04-03T16:06:43.942639Z","shell.execute_reply":"2022-04-03T16:06:43.941892Z","shell.execute_reply.started":"2022-04-03T16:06:43.926279Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:07:59.628367Z","iopub.status.busy":"2022-04-03T16:07:59.627699Z","iopub.status.idle":"2022-04-03T16:07:59.632273Z","shell.execute_reply":"2022-04-03T16:07:59.631542Z","shell.execute_reply.started":"2022-04-03T16:07:59.62833Z"},"trusted":true},"outputs":[],"source":["df_relavent = pd.DataFrame(df['content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:08:21.728518Z","iopub.status.busy":"2022-04-03T16:08:21.728259Z","iopub.status.idle":"2022-04-03T16:08:21.733806Z","shell.execute_reply":"2022-04-03T16:08:21.733041Z","shell.execute_reply.started":"2022-04-03T16:08:21.728488Z"},"trusted":true},"outputs":[],"source":["df_relavent['sentiment'] = df['sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:08:29.23451Z","iopub.status.busy":"2022-04-03T16:08:29.234245Z","iopub.status.idle":"2022-04-03T16:08:29.245362Z","shell.execute_reply":"2022-04-03T16:08:29.244697Z","shell.execute_reply.started":"2022-04-03T16:08:29.234475Z"},"trusted":true},"outputs":[],"source":["df_relavent.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:09:41.132571Z","iopub.status.busy":"2022-04-03T16:09:41.1323Z","iopub.status.idle":"2022-04-03T16:09:41.139611Z","shell.execute_reply":"2022-04-03T16:09:41.138888Z","shell.execute_reply.started":"2022-04-03T16:09:41.13254Z"},"trusted":true},"outputs":[],"source":["df_augmented_all = pd.concat([df_relavent, df_augmented], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:11:49.091913Z","iopub.status.busy":"2022-04-03T16:11:49.091313Z","iopub.status.idle":"2022-04-03T16:11:49.100427Z","shell.execute_reply":"2022-04-03T16:11:49.099705Z","shell.execute_reply.started":"2022-04-03T16:11:49.091874Z"},"trusted":true},"outputs":[],"source":["df_augmented_all.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:09:54.94258Z","iopub.status.busy":"2022-04-03T16:09:54.942324Z","iopub.status.idle":"2022-04-03T16:09:54.949264Z","shell.execute_reply":"2022-04-03T16:09:54.948495Z","shell.execute_reply.started":"2022-04-03T16:09:54.94255Z"},"trusted":true},"outputs":[],"source":["len(df_augmented_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:10:08.241755Z","iopub.status.busy":"2022-04-03T16:10:08.241467Z","iopub.status.idle":"2022-04-03T16:10:08.252784Z","shell.execute_reply":"2022-04-03T16:10:08.252011Z","shell.execute_reply.started":"2022-04-03T16:10:08.241725Z"},"trusted":true},"outputs":[],"source":["df_augmented_all['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T16:12:39.123479Z","iopub.status.busy":"2022-04-03T16:12:39.122755Z","iopub.status.idle":"2022-04-03T16:12:39.162278Z","shell.execute_reply":"2022-04-03T16:12:39.161593Z","shell.execute_reply.started":"2022-04-03T16:12:39.123439Z"},"trusted":true},"outputs":[],"source":["df_augmented_all.to_csv(\"./train_augmented_all.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
