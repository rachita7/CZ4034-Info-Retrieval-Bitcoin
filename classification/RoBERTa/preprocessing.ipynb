{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import unidecode\n",
    "import contractions\n",
    "import tqdm\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from autocorrect import Speller \n",
    "from emot.emo_unicode import UNICODE_EMOJI,EMOTICONS_EMO\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import TransformerDocumentEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from flair.models import TARSClassifier\n",
    "from flair.embeddings import FlairEmbeddings, PooledFlairEmbeddings\n",
    "from flair.data import Dictionary\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /export/home/aneezahm001/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /export/home/aneezahm001/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting emojis to words\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        return text\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = re.sub(u'\\('+emot+'\\)', \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").split()), text)\n",
    "        return text\n",
    "\n",
    "#for now replaced emojis and emoticons instead of removing them\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "    }\n",
    "\n",
    "def accented_characters_removal(text):\n",
    "    # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : Málaga, àéêöhello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text\n",
    "\n",
    "def clean_ascii(text):\n",
    "    # function to remove non-ASCII chars from data\n",
    "    return ''.join(i for i in text if ord(i) < 128)\n",
    "\n",
    "# function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    # define the pattern to keep\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def expand_contractions(text):\n",
    "  return contractions.fix(text)\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    #for now doing in context of verb\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    lemma = \" \".join(lemma)\n",
    "    return lemma\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    text = tweet.lower()\n",
    "   \n",
    "    # Code to remove the Hashtags from the text\n",
    "    text=re.sub(r'\\B#\\S+','',text)\n",
    "\n",
    "    # print(text)\n",
    "    # Code to remove the links from the text\n",
    "    text=re.sub(r\"http\\S+\", \"\",text)\n",
    "    text=re.sub(r\"\\ [A-Za-z]*\\.com\", \" \",text)\n",
    "    # Code to substitute the multiple spaces with single spaces\n",
    "    text=re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # Remove the twitter handlers\n",
    "    text=re.sub('@[^\\s]+','',text)\n",
    "    # print(text)\n",
    "\n",
    "    # text=convert_emoticons(text)\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = text.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "\n",
    "    # text=convert_emojis(text)\n",
    "    #can check which version is better\n",
    "    text = emoji.demojize(text)\n",
    "    # print(text)\n",
    "    text=accented_characters_removal(text)\n",
    "    text=expand_contractions(text)\n",
    "\n",
    "    # Code to remove the Special characters from the text \n",
    "    text=' '.join(re.findall(r'\\w+', text))\n",
    "    # text=remove_numbers(text)\n",
    "\n",
    "    # print(text)\n",
    "    # text=spelling_correction(text)\n",
    "    text=lemmatization(text)\n",
    "    # text=clean_ascii(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/export/home/aneezahm001/IR/data/hand-labelled-old-new-2/inference/train_data_inference.csv\", index_col=0)\n",
    "test = pd.read_csv(\"/export/home/aneezahm001/IR/data/hand-labelled-old-new-2/inference/test_data_inference.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['processed_content', 'sentiment', 'username', 'content', 'date',\n",
       "       'country', 'replyCount', 'retweetCount', 'likeCount', 'url',\n",
       "       'textblob_class', 'vader_class', 'inference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['processed_content', 'sentiment', 'username', 'content', 'date',\n",
       "       'country', 'replyCount', 'retweetCount', 'likeCount', 'url',\n",
       "       'textblob_class', 'vader_class', 'inference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "#Preprocess tweets and save in new column\n",
    "cleantext=[]\n",
    "i=0\n",
    "for item in train['content']:\n",
    "    if type(item)==float:\n",
    "      item = str(item)\n",
    "    words=preprocess_tweet(item)\n",
    "    cleantext+=[words]\n",
    "    i+=1\n",
    "    if i%500==0:\n",
    "      print(i)\n",
    "train['processed_content']=cleantext\n",
    "\n",
    "#Preprocess tweets and save in new column\n",
    "cleantext=[]\n",
    "i=0\n",
    "for item in test['content']:\n",
    "    if type(item)==float:\n",
    "      item = str(item)\n",
    "    words=preprocess_tweet(item)\n",
    "    cleantext+=[words]\n",
    "    i+=1\n",
    "    if i%500==0:\n",
    "      print(i)\n",
    "test['processed_content']=cleantext"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51273a9d1c085b52fc414117ccfdd355a1ae671d0c2292f3db26aa0182437867"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
